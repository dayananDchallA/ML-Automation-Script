{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from missingpy import KNNImputer\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder;\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import tpe, hp, fmin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score,f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Name', axis=1, inplace=True)#must be taken care in EDA\n",
    "df.drop('PassengerId', axis=1, inplace=True)  # must be taken care in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% of counts are \n",
    "\n",
    "class columnTypeIdentification:\n",
    "    def __init__(self, df):\n",
    "        self.dtypes={}\n",
    "        self.df=df\n",
    "        for i in self.df.columns:\n",
    "            self.dtypes[i]=(self.df[i].dtypes)\n",
    "#         print(self.dtypes)\n",
    "        self.colTypes={'Categorical': [], 'Text':[], 'Numeric': []}\n",
    "        self.detectingColTypes()\n",
    "        target_type=''\n",
    "        \n",
    "        for i in self.colTypes.keys():\n",
    "            if y in self.colTypes[i]:\n",
    "                target_type=i\n",
    "                break\n",
    "        self.target_type=target_type\n",
    "    \n",
    "    def detectingColTypes(self):\n",
    "        for i in self.dtypes.keys():\n",
    "            if self.dtypes[i]=='O':\n",
    "                if (df[i].fillna('',axis=0).apply(lambda x: len(x))).quantile(q=0.95)<20:\n",
    "                    self.colTypes['Categorical'].append(i)\n",
    "                else:\n",
    "                    self.colTypes['Text'].append(i)\n",
    "            else: \n",
    "                distinctValues = self.df[i].nunique()\n",
    "                if distinctValues < int((self.df[i].shape[0])*0.05):\n",
    "                    self.colTypes['Categorical'].append(i)\n",
    "                else:\n",
    "                    self.colTypes['Numeric'].append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullHandling:\n",
    "  \n",
    "    def __init__(self, df,colTypes,y):\n",
    "        self.dict_isnull = (df.isnull().sum() / len(df)).to_dict()\n",
    "        self.df=df\n",
    "        self.y=y\n",
    "        self.colTypes=copy.deepcopy(colTypes)\n",
    "        self.remove_columns()\n",
    " \n",
    "    #Removing columns which have more than 75 percent nulls\n",
    "    def remove_columns(self):\n",
    "        cols_remove=[]\n",
    "        for key in self.dict_isnull:\n",
    "            if(self.dict_isnull[key]>0.75):\n",
    "                cols_remove.append(key)        \n",
    "        if not cols_remove:\n",
    "            return self.colTypes\n",
    "        else:\n",
    "            for i in cols_remove:\n",
    "                for j in self.colTypes.keys():\n",
    "                    if i in self.colTypes[j]:\n",
    "                        self.colTypes[j].remove(i)\n",
    "        if self.y in cols_remove:\n",
    "            cols_remove.remove(self.y)\n",
    "        \n",
    "        self.df.drop(cols_remove,axis=1, inplace=True)\n",
    "        \n",
    "#         return self.colTypes\n",
    "        \n",
    "\n",
    "    #Imputing the null values with the mean value of the column\n",
    "    def continuous_impute_mean(self):\n",
    "        df_temp=self.df.copy()\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        df_temp[self.colTypes['Numeric']] = imputer.fit_transform(df_temp[self.colTypes['Numeric']])\n",
    "        return df_temp\n",
    "\n",
    "    #Imputing the null values using KNN\n",
    "    def continuous_impute_knn(self):\n",
    "        df_temp=self.df.copy()\n",
    "        imputer = KNNImputer(n_neighbors=5) \n",
    "        df_temp[self.colTypes['Numeric']] = imputer.fit_transform(df_temp[self.colTypes['Numeric']])\n",
    "        return df_temp\n",
    "    \n",
    "     #Common method calling all the impute functions   \n",
    "    def impute(self,strategy,fill_value = 0, fill_categorical = '-1'):\n",
    "        df_temp=self.df\n",
    "        \n",
    "        #Dealing with Continuous cols\n",
    "        if strategy is None:\n",
    "            df_temp[self.colTypes['Numeric']]=df_temp[self.colTypes['Numeric']].fillna(fill_value)\n",
    "        elif strategy == 'mean':\n",
    "            self.continuous_impute_mean()\n",
    "        elif strategy == 'knn':\n",
    "            self.continuous_impute_knn()\n",
    "            \n",
    "        #dealing with categorical Cols \n",
    "        df_temp[self.colTypes['Categorical']]=df_temp[self.colTypes['Categorical']].fillna(fill_categorical)\n",
    "        \n",
    "        return df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureReduction:\n",
    "\n",
    "    def __init__(self,df,colTypes,y,target_type):\n",
    "        self.df=df\n",
    "        self.colTypes=copy.deepcopy(colTypes)\n",
    "        self.y=y\n",
    "        self.target_type=target_type\n",
    "        self.colTypes[self.target_type].remove(self.y)\n",
    "        #self.all_dfs=[]\n",
    "        self.pearson_corr()\n",
    "\n",
    "    #removing the continuous column with a correlation of above 0.8\n",
    "    def pearson_corr(self):\n",
    "        self.colTypes['Numeric']=set(self.colTypes['Numeric']).intersection(set(self.df.columns))\n",
    "        corr=self.df[list(self.colTypes['Numeric'])].corr(method=\"pearson\").abs()\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > 0.80)]\n",
    "        self.df.drop(to_drop, axis=1,inplace=True)\n",
    "        self.colTypes['Numeric'] = [x for x in self.colTypes['Numeric'] if x not in to_drop]\n",
    "        #self.all_dfs.append(self.df)\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def select_method(self,chooser_):\n",
    "        if(chooser_=='pearson'):\n",
    "            df = self.pearson_corr()\n",
    "            return df\n",
    "            \n",
    "        \n",
    "\n",
    "   # def return_dfs(self):\n",
    "        #return self.all_dfs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OutlierHandling:\n",
    "    def __init__(self, df,colTypes,y,target_type):\n",
    "        self.df = df.copy()\n",
    "        self.y=y\n",
    "        self.target_type=target_type\n",
    "        self.colTypes = copy.deepcopy(colTypes)\n",
    "        self.colTypes[self.target_type].remove(self.y)\n",
    "\n",
    "        self.colTypes['Numeric'] = set(colTypes['Numeric']).intersection(set(df.columns))\n",
    "\n",
    "        #self.all_dfs = []\n",
    "        self.capping_outlier()\n",
    "        self.remove_outlier()\n",
    "        self.zscore_outlier()\n",
    "\n",
    "    # capping the values to the predefined lower and upper percentiles\n",
    "    def capping_outlier(self, lowerperc=0.01, higherperc=0.99):\n",
    "        df = self.df[list(self.colTypes['Numeric'])].copy()\n",
    "        df_out = self.df\n",
    "        for col in df.columns:\n",
    "            percentiles = df[col].quantile([lowerperc, higherperc]).values\n",
    "            df[col][df[col] <= percentiles[0]] = percentiles[0]\n",
    "            df[col][df[col] >= percentiles[1]] = percentiles[1]\n",
    "            df_out[list(self.colTypes['Numeric'])] = df\n",
    "        #self.all_dfs.append(df_out)\n",
    "        \n",
    "        return df_out\n",
    "\n",
    "    # removing the values which are not within iqr range\n",
    "    def remove_outlier(self):\n",
    "        df_out = self.df[list(self.colTypes['Numeric'])].copy()\n",
    "        df_in = self.df.copy(deep=True)\n",
    "        df_in.drop(list(self.colTypes['Numeric']), axis=1, inplace=True)\n",
    "        for col_name in df_out.columns:\n",
    "            q1 = df_out[col_name].quantile(0.25)\n",
    "            q3 = df_out[col_name].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "            df_out = df_out.loc[(df_out[col_name] > lower) & (df_out[col_name] < upper)]\n",
    "        df_final = pd.concat([df_in, df_out], axis=1, join='inner')\n",
    "        #self.all_dfs.append(df_final)\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "    # removing values which are less than predefined zscore value\n",
    "    def zscore_outlier(self, threshold=3):\n",
    "        l = []\n",
    "        df1 = self.df.copy(deep=True)\n",
    "        df = self.df[list(self.colTypes['Numeric'])].copy(deep=True)\n",
    "        df1.drop(self.colTypes['Numeric'], axis=1, inplace=True)\n",
    "        df_out = pd.DataFrame()\n",
    "        for i in df.columns:\n",
    "            temp = []\n",
    "            mean_1 = np.mean(df[i])\n",
    "            std_1 = np.std(df[i])\n",
    "            for y in df[i]:\n",
    "                z_score = (y - mean_1) / std_1\n",
    "                if np.abs(z_score) < threshold:\n",
    "                    temp.append(y)\n",
    "            df_temp = pd.DataFrame(temp)\n",
    "            l.append(df_temp)\n",
    "        df_out = pd.concat(l, axis=1, join='inner')\n",
    "        df_out.columns = df.columns\n",
    "        df_final = pd.concat([df1, df_out], axis=1, join='inner')\n",
    "        #self.all_dfs.append(df_final)\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    def select_method(self,chooser_):\n",
    "        if(chooser_=='removing'):\n",
    "            df=self.remove_outlier()\n",
    "            return df\n",
    "        if(chooser_=='capping'):\n",
    "            df=self.capping_outlier()\n",
    "            return df\n",
    "        if(chooser_=='zscore'):\n",
    "            df=self.zscore_outlier()\n",
    "            return df\n",
    "    \n",
    "    ##def return_dfs(self):\n",
    "        #return self.all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoding:\n",
    "    def __init__(self,df,colTypes,y):\n",
    "        self.y=y\n",
    "        self.colTypes = copy.deepcopy(colTypes)\n",
    "        # fetching column types for the columns in the current df\n",
    "        self.colTypes['Categorical'] = set(df.columns).intersection(set(colTypes['Categorical']))\n",
    "        # removing the target column for the list of categorical columns\n",
    "        if self.y in self.colTypes['Categorical']:\n",
    "            self.colTypes['Categorical'].remove(self.y)\n",
    "\n",
    "        self.df = df.copy()\n",
    "        #self.all_dfs = []\n",
    "        self.one_hot_encoding()\n",
    "        self.label_encode()\n",
    "\n",
    "    # encoding the categorical columns excluding the target column\n",
    "    def one_hot_encoding(self):\n",
    "        df1 = self.df.copy(deep=True)\n",
    "        df_y = pd.DataFrame()\n",
    "        df1 = pd.get_dummies(df1, drop_first=True, columns=list(self.colTypes['Categorical']))\n",
    "        #self.all_dfs.append(self.target_encode(df1))\n",
    "        \n",
    "        return self.target_encode(df1)\n",
    "\n",
    "    # encoding the categorical columns excluding the target column\n",
    "    def label_encode(self):\n",
    "        df1 = self.df.copy(deep=True)\n",
    "        df_y = pd.DataFrame()\n",
    "        for x in self.colTypes['Categorical']:\n",
    "            df1[x] = LabelEncoder.fit_transform(df1, y=df1[x])\n",
    "        #self.all_dfs.append(self.target_encode(df1))\n",
    "        \n",
    "        return self.target_encode(df1)\n",
    "\n",
    "    # encoding the categorical target column\n",
    "    def target_encode(self, t_df):\n",
    "        if self.y not in self.colTypes['Numeric']:\n",
    "            t_df[self.y] = LabelEncoder.fit_transform(t_df, y=t_df[self.y])\n",
    "        return t_df\n",
    "       \n",
    "        \n",
    "    def select_method(self,chooser_):\n",
    "        if(chooser_=='one_hot'):\n",
    "            df=self.one_hot_encode()\n",
    "            return df\n",
    "        if(chooser_=='label_encode'):\n",
    "            df = self.label_encode()\n",
    "            return df\n",
    "\n",
    "        \n",
    "    ##def return_dfs(self):\n",
    "       # return self.all_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import tpe, hp, fmin, space_eval, STATUS_OK, Trials\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Parameters\n",
    "#List of dfs, targetCol and the metric under consideration\n",
    "\n",
    "class Classification:\n",
    "    def __init__(self, df, targetCol, metric, colTypes, test_size=0.2):\n",
    "        self.df = df\n",
    "        self.y = targetCol\n",
    "        self.metric = metric\n",
    "        self.colTypes=colTypes\n",
    "        self.test_size=test_size\n",
    "        # self.test_score = {}\n",
    "        self.final_results={}\n",
    "\n",
    "        self.execute()\n",
    "\n",
    "    #Required to be passed in fmin of hyperopt\n",
    "    def objective_func(self, args):\n",
    "        clf=None\n",
    "\n",
    "        if args['model'] == RandomForestClassifier:\n",
    "            n_estimators = args['param']['n_estimators']\n",
    "            max_depth = args['param']['max_depth']\n",
    "            max_features = args['param']['max_features']\n",
    "            clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n",
    "        elif args['model'] == LogisticRegression:\n",
    "            penalty = args['param']['penalty']\n",
    "            C = args['param']['C']\n",
    "            solver = args['param']['solver']\n",
    "            clf = LogisticRegression(penalty=penalty, C=C, solver=solver)\n",
    "\n",
    "        clf.fit(self.x_train, self.y_train)\n",
    "        y_pred_train = clf.predict(self.x_train)\n",
    "        y_pred_test = clf.predict(self.x_test)\n",
    "        loss = log_loss(self.y_train, y_pred_train)\n",
    "        score = self.metric(y_pred_test, self.y_test)\n",
    "        # print(\"Test Score:\", self.metric(y_pred_test, self.y_test))\n",
    "        # print(\"Train Score:\", self.metric(y_pred_train, self.y_train))\n",
    "        # print(\"\\n===============\")\n",
    "        # return {'loss': score,'status': STATUS_OK,'model': clf}\n",
    "        return (-score)\n",
    "\n",
    "    def execute(self):\n",
    "        #using Hyperopt for parameter tuning\n",
    "        self.space = hp.choice('classifier', [\n",
    "                                            {'model': RandomForestClassifier,\n",
    "                                            'param': {'max_depth': hp.choice('max_depth', range(1, 20)),\n",
    "                                            'max_features': hp.choice('max_features', range(1, 5)),\n",
    "                                            'n_estimators': hp.choice('n_estimators', range(1, 20)),\n",
    "                                             'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n",
    "                                                    }\n",
    "                                            },\n",
    "                                            {'model': LogisticRegression,\n",
    "                                             'param': {'penalty': hp.choice('penalty', ['l2']),\n",
    "                                                       'C': hp.lognormal('C', 0, 1),\n",
    "                                                       'solver': hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])}\n",
    "                                             }\n",
    "                                        ])\n",
    "\n",
    "        score_key = str(self.metric) + '_score'\n",
    "\n",
    "\n",
    "            #df.drop(self.colTypes['Text'], axis=1, inplace=True)#must be taken care in EDA\n",
    "            #df.drop('PassengerId', axis=1, inplace=True)  # must be taken care in EDA\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.df.drop(self.y, axis=1), self.df[self.y], test_size=self.test_size)\n",
    "        trials = Trials()\n",
    "\n",
    "        hyperparam = space_eval(self.space,fmin(self.objective_func, self.space, trials=trials, algo=tpe.suggest, max_evals=100))\n",
    "        score = -min(trials.losses())\n",
    "\n",
    "\n",
    "        self.final_results['Hyperparameter']=hyperparam\n",
    "        self.final_results[score_key]=score\n",
    "        #self.final_results['Data']=self.df\n",
    "\n",
    "\n",
    "    def return_results(self):\n",
    "        return self.final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import threading\n",
    "s=[ [True], ['none','mean'], ['e', 'f'] ]\n",
    "\n",
    "#s=[[True],[None,'mean','knn'],['pearson'],['capping','removing','zscore'],['one_hot','label_encode','target_encode']]\n",
    "x=list(itertools.product(*s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TruenoneTrue\n",
      "\n",
      "\n",
      "noneTruee\n",
      "mean\n",
      "\n",
      "\n",
      "meanf\n",
      "e\n",
      "\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def print_(queue,a,b,c):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "cnt_=0\n",
    "for list_ in x:\n",
    "    #cnt_=cnt_+1\n",
    "    #a, b, c = [list_[i] for i in range(len(list_))] \n",
    "    #func_=partial(threading_,df)\n",
    "    #t1 = threading.Thread(target=func_,args=([list_[i] for i in range(len(list_))]))\n",
    "    #z=1\n",
    "    func_=partial(print_,queue)\n",
    "    t1 = threading.Thread(target=func_,args=([list_[i] for i in range(len(list_))]))  \n",
    "    t1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=accuracy_score\n",
    "\n",
    "class FlowClass:\n",
    "    def __init__(self,df,y,cnt_):\n",
    "        self.df=df\n",
    "        self.y=y\n",
    "        self.cnt_=cnt_\n",
    "\n",
    "    def threading_(self,queue,col_identification,null_handling,feature_reduction,outlier_handling,encoding,modelling):\n",
    "        df=self.df.copy()\n",
    "        y=self.y\n",
    "        #cnt_=self.cnt_\n",
    "        path=[]\n",
    "        d_ = {}\n",
    "        \n",
    "        if(col_identification):\n",
    "            colIdentObj = columnTypeIdentification(df)\n",
    "            colTypes = colIdentObj.colTypes\n",
    "            target_type = colIdentObj.target_type\n",
    "            z='col_identification'\n",
    "            path.append(z)\n",
    "\n",
    "            \n",
    "        if(null_handling!='Null'):\n",
    "            nullHndlngObj = NullHandling(df,colTypes,y)\n",
    "            df = nullHndlngObj.impute(null_handling)\n",
    "            #z='null_handling_df'+str(cnt_)+'.csv'\n",
    "            #df.to_csv('null_handling_df.csv')\n",
    "            z=null_handling\n",
    "            path.append(z)\n",
    "            \n",
    "            \n",
    "        if(feature_reduction!='Null'):\n",
    "            fRdctionObj = FeatureReduction(df,colTypes,y,target_type)\n",
    "            df=fRdctionObj.select_method(feature_reduction)\n",
    "            #df.to_csv('feature_reduction_df.csv')\n",
    "            z=feature_reduction\n",
    "            path.append(z)\n",
    "        \n",
    "        \n",
    "        if(outlier_handling!= 'Null'):\n",
    "            OH = OutlierHandling(df,colTypes,y,target_type)\n",
    "            df=OH.select_method(outlier_handling)\n",
    "            #df.to_csv('outlier_handling_df.csv')\n",
    "            z=outlier_handling\n",
    "            path.append(z)\n",
    "            \n",
    "        \n",
    "        if(encoding!= 'Null'):\n",
    "            en = Encoding(df,colTypes,y)\n",
    "            df=en.select_method(encoding)\n",
    "            #df.to_csv('encoding_df.csv')\n",
    "            z=encoding\n",
    "            path.append(z)\n",
    "            \n",
    "            \n",
    "        if(modelling!='Null'):\n",
    "            metric = modelling\n",
    "            clf = Classification(df,y,metric,colTypes,test_size = 0.2)\n",
    "            param=clf.return_results()\n",
    "            z=modelling\n",
    "            path.append(z)\n",
    "        \n",
    "        d_['df']=df\n",
    "        d_['path']=path\n",
    "        d_['model_results'] = param\n",
    "        queue.put(d_)\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:236: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  self.classes_, y = _encode(y, encode=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/100 [00:00<?, ?trial/s, best loss=?]\n",
      "                                                                                                                       \n",
      "  1%|▍                                              | 1/100 [00:00<00:14,  6.66trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|9                                              | 2/100 [00:00<00:10,  9.60trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      "  2%|▉                                              | 2/100 [00:00<00:14,  6.66trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n",
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|#8                                             | 4/100 [00:00<00:10,  9.39trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      "  2%|▉                                              | 2/100 [00:00<00:14,  6.66trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|##3                                            | 5/100 [00:00<00:11,  8.35trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      "  3%|█▍                                             | 3/100 [00:00<00:16,  5.83trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|##8                                            | 6/100 [00:00<00:13,  7.02trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      "  5%|██▎                                            | 5/100 [00:00<00:16,  5.89trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7%|###2                                           | 7/100 [00:00<00:14,  6.50trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      "  7%|███▎                                           | 7/100 [00:01<00:13,  6.68trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9%|####2                                          | 9/100 [00:01<00:12,  7.54trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      "  8%|███▊                                           | 8/100 [00:01<00:13,  6.77trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|####6                                         | 10/100 [00:01<00:11,  7.71trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      "  8%|███▊                                           | 8/100 [00:01<00:13,  6.77trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|#####                                         | 11/100 [00:01<00:13,  6.82trial/s, best loss: -0.7786885245901639]\n",
      "                                                                                                                       \n",
      " 13%|######1                                        | 13/100 [00:01<00:10,  8.12trial/s, best loss: -0.819672131147541]\n",
      "                                                                                                                       \n",
      "  8%|███▊                                           | 8/100 [00:01<00:13,  6.77trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█████▌                                        | 12/100 [00:01<00:16,  5.45trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|######5                                        | 14/100 [00:01<00:16,  5.14trial/s, best loss: -0.819672131147541]\n",
      "                                                                                                                       \n",
      " 14%|██████▍                                       | 14/100 [00:02<00:12,  6.79trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15%|#######                                        | 15/100 [00:02<00:15,  5.40trial/s, best loss: -0.819672131147541]\n",
      "                                                                                                                       \n",
      " 14%|██████▍                                       | 14/100 [00:02<00:12,  6.79trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17%|#######9                                       | 17/100 [00:02<00:12,  6.49trial/s, best loss: -0.819672131147541]\n",
      "                                                                                                                       \n",
      " 14%|██████▍                                       | 14/100 [00:02<00:12,  6.79trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19%|########9                                      | 19/100 [00:02<00:10,  7.69trial/s, best loss: -0.819672131147541]\n",
      "                                                                                                                       \n",
      " 21%|#########8                                     | 21/100 [00:02<00:08,  8.95trial/s, best loss: -0.819672131147541]\n",
      "                                                                                                                       \n",
      " 23%|##########5                                   | 23/100 [00:02<00:07, 10.17trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 25%|###########5                                  | 25/100 [00:02<00:06, 11.17trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 27%|############4                                 | 27/100 [00:02<00:06, 11.71trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 14%|██████▍                                       | 14/100 [00:03<00:12,  6.79trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29%|#############3                                | 29/100 [00:03<00:06, 11.50trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 31%|##############2                               | 31/100 [00:03<00:06, 11.25trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 33%|###############1                              | 33/100 [00:03<00:05, 12.30trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 35%|################                              | 35/100 [00:03<00:05, 11.68trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 37%|#################                             | 37/100 [00:03<00:05, 11.16trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 15%|██████▉                                       | 15/100 [00:03<00:12,  6.79trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39%|#################9                            | 39/100 [00:04<00:06,  9.74trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 41%|##################8                           | 41/100 [00:04<00:05, 10.03trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 43%|###################7                          | 43/100 [00:04<00:05, 10.47trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 16%|███████▎                                      | 16/100 [00:04<00:34,  2.42trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▊                                      | 17/100 [00:04<00:38,  2.15trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|████████▋                                     | 19/100 [00:04<00:28,  2.85trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45%|####################7                         | 45/100 [00:05<00:08,  6.65trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 47%|#####################6                        | 47/100 [00:05<00:08,  6.57trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 48%|######################                        | 48/100 [00:05<00:09,  5.41trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 49%|######################5                       | 49/100 [00:05<00:08,  6.11trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 30%|█████████████▊                                | 30/100 [00:06<00:10,  6.91trial/s, best loss: -0.8516129032258064]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50%|#######################                       | 50/100 [00:06<00:11,  4.33trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 51%|#######################4                      | 51/100 [00:06<00:11,  4.41trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 52%|#######################9                      | 52/100 [00:06<00:11,  4.13trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 38%|█████████████████▊                             | 38/100 [00:06<00:06,  9.24trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53%|########################3                     | 53/100 [00:06<00:12,  3.72trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 54%|########################8                     | 54/100 [00:07<00:11,  4.05trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 56%|#########################7                    | 56/100 [00:07<00:08,  4.98trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 57%|##########################2                   | 57/100 [00:07<00:11,  3.75trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 58%|##########################6                   | 58/100 [00:07<00:09,  4.60trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 59%|###########################1                  | 59/100 [00:08<00:08,  4.59trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 61%|############################                  | 61/100 [00:08<00:07,  5.39trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 62%|############################5                 | 62/100 [00:08<00:06,  5.65trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 63%|############################9                 | 63/100 [00:08<00:07,  4.95trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 64%|#############################4                | 64/100 [00:08<00:06,  5.33trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 50%|███████████████████████▌                       | 50/100 [00:09<00:07,  6.38trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65%|#############################9                | 65/100 [00:09<00:07,  4.47trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 66%|##############################3               | 66/100 [00:09<00:06,  5.25trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 50%|███████████████████████▌                       | 50/100 [00:09<00:07,  6.38trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67%|##############################8               | 67/100 [00:09<00:05,  5.73trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 68%|###############################2              | 68/100 [00:09<00:05,  6.09trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 70%|################################1             | 70/100 [00:09<00:04,  6.61trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 71%|################################6             | 71/100 [00:09<00:04,  6.70trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 55%|█████████████████████████▊                     | 55/100 [00:10<00:07,  6.16trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72%|#################################1            | 72/100 [00:10<00:05,  5.33trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 73%|#################################5            | 73/100 [00:10<00:05,  5.33trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 74%|##################################            | 74/100 [00:10<00:04,  5.91trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 75%|##################################5           | 75/100 [00:10<00:03,  6.69trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 76%|##################################9           | 76/100 [00:10<00:03,  7.17trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 78%|###################################8          | 78/100 [00:10<00:02,  8.43trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 80%|####################################8         | 80/100 [00:11<00:02,  9.63trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 59%|███████████████████████████▋                   | 59/100 [00:11<00:05,  7.42trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████▍               | 67/100 [00:11<00:05,  6.41trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82%|#####################################7        | 82/100 [00:11<00:02,  6.09trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 83%|######################################1       | 83/100 [00:11<00:02,  5.90trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 84%|######################################6       | 84/100 [00:11<00:02,  6.12trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 86%|#######################################5      | 86/100 [00:12<00:02,  5.81trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 87%|########################################      | 87/100 [00:12<00:02,  6.44trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 88%|########################################4     | 88/100 [00:12<00:01,  6.66trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 89%|########################################9     | 89/100 [00:12<00:01,  6.89trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 91%|#########################################8    | 91/100 [00:12<00:01,  8.12trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 92%|##########################################3   | 92/100 [00:13<00:01,  7.71trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 93%|##########################################7   | 93/100 [00:13<00:00,  7.70trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 94%|###########################################2  | 94/100 [00:13<00:00,  6.32trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 83%|███████████████████████████████████████        | 83/100 [00:14<00:02,  8.10trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95%|###########################################6  | 95/100 [00:14<00:01,  3.20trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 96%|############################################1 | 96/100 [00:14<00:01,  3.79trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 97%|############################################6 | 97/100 [00:14<00:00,  4.27trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 99%|#############################################5| 99/100 [00:14<00:00,  5.47trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      "100%|#############################################| 100/100 [00:14<00:00,  6.85trial/s, best loss: -0.8360655737704918]\n",
      "                                                                                                                       \n",
      " 85%|███████████████████████████████████████▉       | 85/100 [00:14<00:01,  8.07trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████▏   | 92/100 [00:15<00:00,  9.17trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████ | 98/100 [00:15<00:00, 12.77trial/s, best loss: -0.864516129032258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SindhuKarnati\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [00:15<00:00, 13.01trial/s, best loss: -0.864516129032258]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import threading\n",
    "import queue\n",
    "from functools import partial\n",
    "\n",
    "#s=[[True],[None,'mean','knn'],['pearson'],['capping','removing','zscore'],['one_hot','label_encode']]\n",
    "s=[[True],[None,'mean'],['Null'],['removing'],['label_encode'],[accuracy_score]]\n",
    "x=list(itertools.product(*s))\n",
    "\n",
    "\n",
    "y = 'Survived'\n",
    "cnt_=0\n",
    "queue = queue.Queue()\n",
    "thread_list = []\n",
    "f_list = []\n",
    "for list_ in x:\n",
    "    cnt_ = cnt_+1\n",
    "    obj = FlowClass(df,y,cnt_)\n",
    "    func_=partial(obj.threading_,queue)\n",
    "    t1 = threading.Thread(target=func_,args=([list_[i] for i in range(len(list_))]))\n",
    "    t1.start()\n",
    "    thread_list.append(t1)\n",
    "\n",
    "\n",
    "for t in thread_list:\n",
    "    t.join()\n",
    "    #df = queue.get()\n",
    "    #z = 'final_df'+str(ct_)+'.csv'\n",
    "    #df.to_csv(z)\n",
    "    f_list.append(queue.get())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'df':      Survived  Pclass  Sex  SibSp  Parch  Ticket  Embarked   Age     Fare\n",
       "  0           0       2    1      1      0     375         2  22.0   7.2500\n",
       "  2           1       2    0      0      0     479         2  26.0   7.9250\n",
       "  3           1       0    0      1      0      32         2  35.0  53.1000\n",
       "  4           0       2    1      0      0     340         2  35.0   8.0500\n",
       "  6           0       0    1      0      0      52         2  54.0  51.8625\n",
       "  7           0       2    1      3      1     294         2   2.0  21.0750\n",
       "  8           1       2    0      0      2     258         2  27.0  11.1333\n",
       "  9           1       1    0      1      0      91         0  14.0  30.0708\n",
       "  10          1       2    0      1      1     436         2   4.0  16.7000\n",
       "  11          1       0    0      0      0      22         2  58.0  26.5500\n",
       "  12          0       2    1      0      0     385         2  20.0   8.0500\n",
       "  13          0       2    1      1      5     247         2  39.0  31.2750\n",
       "  14          0       2    0      0      0     312         2  14.0   7.8542\n",
       "  15          1       1    0      0      0     106         2  55.0  16.0000\n",
       "  16          0       2    1      4      1     344         1   2.0  29.1250\n",
       "  18          0       2    0      1      0     217         2  31.0  18.0000\n",
       "  20          0       1    1      0      0      94         2  35.0  26.0000\n",
       "  21          1       1    1      0      0     105         2  34.0  13.0000\n",
       "  22          1       2    0      0      0     206         1  15.0   8.0292\n",
       "  23          1       0    1      0      0      26         2  28.0  35.5000\n",
       "  24          0       2    0      3      1     294         2   8.0  21.0750\n",
       "  25          1       2    0      1      5     243         2  38.0  31.3875\n",
       "  30          0       0    1      0      0     431         0  40.0  27.7208\n",
       "  35          0       0    1      1      0      27         2  42.0  52.0000\n",
       "  37          0       2    1      0      0     368         2  21.0   8.0500\n",
       "  38          0       2    0      2      0     218         2  18.0  18.0000\n",
       "  39          1       2    0      1      0     132         0  14.0  11.2417\n",
       "  40          0       2    0      1      0     362         2  40.0   9.4750\n",
       "  41          0       1    0      1      0      36         2  27.0  21.0000\n",
       "  43          1       1    0      1      2     449         0   3.0  41.5792\n",
       "  ..        ...     ...  ...    ...    ...     ...       ...   ...      ...\n",
       "  854         0       1    0      1      0      98         2  44.0  26.0000\n",
       "  855         1       2    0      0      1     345         2  18.0   9.3500\n",
       "  857         1       0    1      0      0      15         2  51.0  26.5500\n",
       "  858         1       2    0      0      3     137         0  24.0  19.2583\n",
       "  860         0       2    1      2      0     298         2  41.0  14.1083\n",
       "  861         0       1    1      1      0     156         2  21.0  11.5000\n",
       "  862         1       0    0      0      0      54         2  48.0  25.9292\n",
       "  864         0       1    1      0      0      79         2  24.0  13.0000\n",
       "  865         1       1    0      0      0      85         2  42.0  13.0000\n",
       "  866         1       1    0      1      0     447         0  27.0  13.8583\n",
       "  867         0       0    1      0      0     426         2  31.0  50.4958\n",
       "  869         1       2    1      1      1     258         2   4.0  11.1333\n",
       "  870         0       2    1      0      0     288         2  26.0   7.8958\n",
       "  871         1       0    0      1      1      37         2  47.0  52.5542\n",
       "  872         0       0    1      0      0     357         2  33.0   5.0000\n",
       "  873         0       2    1      0      0     219         2  47.0   9.0000\n",
       "  874         1       1    0      1      0     418         0  28.0  24.0000\n",
       "  875         1       2    0      0      0     138         0  15.0   7.2250\n",
       "  876         0       2    1      0      0     359         2  20.0   9.8458\n",
       "  877         0       2    1      0      0     270         2  19.0   7.8958\n",
       "  880         1       1    0      0      1      74         2  25.0  26.0000\n",
       "  881         0       2    1      0      0     293         2  33.0   7.8958\n",
       "  882         0       2    0      0      0     363         2  22.0  10.5167\n",
       "  883         0       1    1      0      0     410         2  28.0  10.5000\n",
       "  884         0       2    1      0      0     462         2  25.0   7.0500\n",
       "  885         0       2    0      0      5     344         1  39.0  29.1250\n",
       "  886         0       1    1      0      0      61         2  27.0  13.0000\n",
       "  887         1       0    0      0      0       9         2  19.0  30.0000\n",
       "  889         1       0    1      0      0       5         0  26.0  30.0000\n",
       "  890         0       2    1      0      0     338         1  32.0   7.7500\n",
       "  \n",
       "  [607 rows x 9 columns],\n",
       "  'path': ['col_identification',\n",
       "   'mean',\n",
       "   'pearson',\n",
       "   'removing',\n",
       "   'label_encode',\n",
       "   <function sklearn.metrics.classification.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)>],\n",
       "  'model_results': {'Hyperparameter': {'model': sklearn.ensemble.forest.RandomForestClassifier,\n",
       "    'param': {'criterion': 'entropy',\n",
       "     'max_depth': 13,\n",
       "     'max_features': 2,\n",
       "     'n_estimators': 8}},\n",
       "   '<function accuracy_score at 0x000001E3CA2077B8>_score': 0.8360655737704918}},\n",
       " {'df':      Survived  Pclass  Sex  SibSp  Parch  Ticket  Embarked   Age     Fare\n",
       "  0           0       2    1      1      0     495         2  22.0   7.2500\n",
       "  2           1       2    0      0      0     621         2  26.0   7.9250\n",
       "  3           1       0    0      1      0      41         2  35.0  53.1000\n",
       "  4           0       2    1      0      0     444         2  35.0   8.0500\n",
       "  5           0       2    1      0      0     253         1   0.0   8.4583\n",
       "  6           0       0    1      0      0      68         2  54.0  51.8625\n",
       "  7           0       2    1      3      1     372         2   2.0  21.0750\n",
       "  8           1       2    0      0      2     321         2  27.0  11.1333\n",
       "  9           1       1    0      1      0     111         0  14.0  30.0708\n",
       "  10          1       2    0      1      1     569         2   4.0  16.7000\n",
       "  11          1       0    0      0      0      30         2  58.0  26.5500\n",
       "  12          0       2    1      0      0     507         2  20.0   8.0500\n",
       "  13          0       2    1      1      5     310         2  39.0  31.2750\n",
       "  14          0       2    0      0      0     390         2  14.0   7.8542\n",
       "  15          1       1    0      0      0     131         2  55.0  16.0000\n",
       "  16          0       2    1      4      1     452         1   2.0  29.1250\n",
       "  17          1       1    1      0      0     129         2   0.0  13.0000\n",
       "  18          0       2    0      1      0     278         2  31.0  18.0000\n",
       "  19          1       2    0      0      0     162         0   0.0   7.2250\n",
       "  20          0       1    1      0      0     118         2  35.0  26.0000\n",
       "  21          1       1    1      0      0     130         2  34.0  13.0000\n",
       "  22          1       2    0      0      0     256         1  15.0   8.0292\n",
       "  23          1       0    1      0      0      34         2  28.0  35.5000\n",
       "  24          0       2    0      3      1     372         2   8.0  21.0750\n",
       "  25          1       2    0      1      5     306         2  38.0  31.3875\n",
       "  26          0       2    1      0      0     157         0   0.0   7.2250\n",
       "  28          1       2    0      0      0     261         1   0.0   7.8792\n",
       "  29          0       2    1      0      0     339         2   0.0   7.8958\n",
       "  30          0       0    1      0      0     560         0  40.0  27.7208\n",
       "  32          1       2    0      0      0     266         1   0.0   7.7500\n",
       "  ..        ...     ...  ...    ...    ...     ...       ...   ...      ...\n",
       "  859         0       2    1      0      0     156         0   0.0   7.2292\n",
       "  860         0       2    1      2      0     376         2  41.0  14.1083\n",
       "  861         0       1    1      1      0     199         2  21.0  11.5000\n",
       "  862         1       0    0      0      0      71         2  48.0  25.9292\n",
       "  864         0       1    1      0      0      99         2  24.0  13.0000\n",
       "  865         1       1    0      0      0     105         2  42.0  13.0000\n",
       "  866         1       1    0      1      0     584         0  27.0  13.8583\n",
       "  867         0       0    1      0      0     554         2  31.0  50.4958\n",
       "  868         0       2    1      0      0     286         2   0.0   9.5000\n",
       "  869         1       2    1      1      1     321         2   4.0  11.1333\n",
       "  870         0       2    1      0      0     364         2  26.0   7.8958\n",
       "  871         1       0    0      1      1      46         2  47.0  52.5542\n",
       "  872         0       0    1      0      0     474         2  33.0   5.0000\n",
       "  873         0       2    1      0      0     280         2  47.0   9.0000\n",
       "  874         1       1    0      1      0     546         0  28.0  24.0000\n",
       "  875         1       2    0      0      0     173         0  15.0   7.2250\n",
       "  876         0       2    1      0      0     476         2  20.0   9.8458\n",
       "  877         0       2    1      0      0     335         2  19.0   7.8958\n",
       "  878         0       2    1      0      0     340         2   0.0   7.8958\n",
       "  880         1       1    0      0      1      94         2  25.0  26.0000\n",
       "  881         0       2    1      0      0     371         2  33.0   7.8958\n",
       "  882         0       2    0      0      0     480         2  22.0  10.5167\n",
       "  883         0       1    1      0      0     537         2  28.0  10.5000\n",
       "  884         0       2    1      0      0     602         2  25.0   7.0500\n",
       "  885         0       2    0      0      5     452         1  39.0  29.1250\n",
       "  886         0       1    1      0      0      80         2  27.0  13.0000\n",
       "  887         1       0    0      0      0      11         2  19.0  30.0000\n",
       "  888         0       2    0      1      2     627         2   0.0  23.4500\n",
       "  889         1       0    1      0      0       5         0  26.0  30.0000\n",
       "  890         0       2    1      0      0     438         1  32.0   7.7500\n",
       "  \n",
       "  [775 rows x 9 columns],\n",
       "  'path': ['col_identification',\n",
       "   None,\n",
       "   'pearson',\n",
       "   'removing',\n",
       "   'label_encode',\n",
       "   <function sklearn.metrics.classification.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)>],\n",
       "  'model_results': {'Hyperparameter': {'model': sklearn.ensemble.forest.RandomForestClassifier,\n",
       "    'param': {'criterion': 'entropy',\n",
       "     'max_depth': 7,\n",
       "     'max_features': 2,\n",
       "     'n_estimators': 8}},\n",
       "   '<function accuracy_score at 0x000001E3CA2077B8>_score': 0.864516129032258}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
